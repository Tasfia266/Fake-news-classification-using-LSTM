# -*- coding: utf-8 -*-
"""Fake_news_classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BEOZWApnjPSEDLfb_vQ7U31PGHMw4vcV
"""

from google.colab import drive 
drive.mount ('/content/drive')

import pandas as pd
df= pd.read_csv ('/content/drive/MyDrive/train.csv')

df.head(10)

# drop nan values 
df= df.dropna()

df.shape

# Get the features 
x= df.drop ('label', axis=1)
x.shape

y= df['label']
y.shape

import tensorflow as tf

from tensorflow.keras.layers import Embedding 
from tensorflow.keras.preprocessing.sequence import pad_sequences 
from tensorflow.keras.models import Sequential 
from tensorflow.keras.preprocessing.text import one_hot 
from tensorflow.keras.layers import LSTM 
from tensorflow.keras.layers import Dense

# vocabulary size 
voc_size= 5000

message= x.copy() 
message['title'][50]

message.reset_index (inplace=True) # Because we have dropped some nan values, we have to reset the index

import nltk # natural language toolkit for natural language processing in python 
import re 
from nltk.corpus import stopwords # In natural language processing, useless words referred to as stopwords

nltk.download ('stopwords')

from nltk.stem.porter import PorterStemmer # stemming is basically removing a suffix from a word and reduce it to its root word 
# why do we need stemming? The main aim is to reduce inflectional forms of each word into a common base word or root word and stem word 
ps= PorterStemmer() 
corpus=[] 
for i in range (0, len (message)): 
  review= re.sub('[^a-zA-Z]',' ', message['title'][i]) # apart from the character a-z, A-Z, substitute everything with a blank
  review= review.lower() 
  review= review.split () # because we need to do stopwords
  review= [ps.stem (word) for word in review if not word in stopwords.words ('english')]
  review= ' '.join(review)
  corpus.append (review)

corpus[1]

one_hot_representation= [one_hot (words, voc_size) for words in corpus]

one_hot_representation

# embedding layers 
sent_length=20 
embedded_docs= pad_sequences (one_hot_representation, padding='pre', maxlen=sent_length)

embedded_docs[0]

embedding_vector_features=40 
model= Sequential () 
model.add(Embedding (voc_size, embedding_vector_features,input_length=sent_length))
model.add(LSTM(100)) #100 neurons
model.add(Dense (1, activation='sigmoid'))

model.compile (optimizer='adam', loss= 'binary_crossentropy', metrics=['accuracy'])

model.summary()

import numpy as np

x_final= np.array (embedded_docs)
y_final= np.array (y)

x_final.shape, y_final.shape

# train test dataset creating 
from sklearn.model_selection import train_test_split 
x_train, x_test, y_train, y_test= train_test_split (x_final, y_final, test_size=0.2, random_state=42)

history= model.fit (x_train, y_train, epochs=20, validation_data=(x_test, y_test), batch_size=64, verbose=2)

y_pred= model.predict(x_test)

y_pred[0:5], y_test[0:5]

model.evaluate (x_test, y_test)

from tensorflow.keras.layers import Bidirectional

# Training with Bi-directional LSTM

model_2= Sequential () 
model_2.add(Embedding (voc_size, embedding_vector_features, input_length= sent_length)) 
model_2.add(Bidirectional (LSTM (100))) 
model_2.add(Dense(1, activation="sigmoid"))

model_2.compile (optimizer= "adam", loss= "binary_crossentropy", metrics=["accuracy"])

model_2.summary()

history2= model_2.fit (x_train, y_train, epochs=20, validation_data= (x_test, y_test),batch_size=64, verbose=2)

predict_y= model_2.predict (x_test)

predict_y[0:5], y_test[0:5]

"{:.8f}".format (float (predict_y[1]))

#see the classification report